//-----------------------------------------------------------------------------------------//
###  - 1 -
31-12-21

"""
model.compile(optimizer="rmsprop", loss="sparse_categorical_crossentropy")
"""

binarización con //255 (solo se consideran los pixeles blancos, umbral=255)
15 epochs
N (total samples) = 1500
train%samples 0.7
batch size = 2
Val_size: 450 samples

IOU total: 344
IOU avg: 0.764

###
//-----------------------------------------------------------------------------------------//
###  - 2 -
31-12-21

"""
model.compile(
        optimizer=tf.keras.optimizers.SGD(lr=0.01, momentum=0.9, clipnorm=1.0),
        loss=tf.keras.losses.BinaryCrossentropy(from_logits=False)
    )
"""

binarización con > 128 (umbral=128)
15 epochs
N (total samples) = 1500
train%samples 0.7
batch size = 2
Val_size: 450 samples

IOU total: 25.17
IOU avg: 0.05593

Notes:
Obtuve una rejilla con clas predicciones donde se distinguian algunas formas (no donde las manos)
El modelo necesita cambios para adaptar a solo dos predicciones (numclasses=1?)
###
//-----------------------------------------------------------------------------------------//
###  - 3 -
31-12-21

"""
model.compile(optimizer="rmsprop", loss="sparse_categorical_crossentropy")
"""

binarización con > 128 (umbral=128)
15 epochs
N (total samples) = 1500
train%samples 0.7
batch size = 2
Val_size: 450 samples

IOU total: 355.42
IOU avg: 0.78982

###
//-----------------------------------------------------------------------------------------//
###  - 3 -
31-12-21

"""
model.compile(
    optimizer=tf.keras.optimizers.SGD(lr=0.01, momentum=0.9, clipnorm=1.0),
    loss="sparse_categorical_crossentropy")
"""

binarización con > 128 (umbral=128)
15 epochs
N (total samples) = 1500
train%samples 0.7
batch size = 2
Val_size: 450 samples

IOU total: 353.29
IOU avg: 0.78508

###
//-----------------------------------------------------------------------------------------//
###  - 4 -
31-12-21

"""
model.compile(optimizer = keras.optimizers.Adam(), loss='binary_crossentropy',
            metrics = ['accuracy'])
"""

binarización con > 128 (umbral=128)
10 epochs
N (total samples) = 1500
train%samples 0.7
batch size = 2
Val_size: 450 samples

IOU total: 356.97
IOU avg: 0.79326

###
//-----------------------------------------------------------------------------------------//
###  - 5 -
31-12-21

"""
model.compile(optimizer = keras.optimizers.Adam(), loss='binary_crossentropy',
            metrics = ['accuracy'])
"""

binarización con > 128 (umbral=128)
15 epochs
N (total samples) = 1500
train%samples 0.7
batch size = 2
Val_size: 450 samples

IOU total: 363.94
IOU avg: 0.8087555

###
//-----------------------------------------------------------------------------------------//
###  - 6 -
12-02-22

"""
model.compile(optimizer = keras.optimizers.Adam(), 
			loss = pixelwise_binary_cross_entropy_tf, 
			eval_loss='binary_crossentropy',
            metrics = ['accuracy'])
"""

binarización con > 128 (umbral=128)
11 epochs
N (total samples) = 1500
train%samples 0.7
batch size = 1
Val_size: 450 samples

IOU total: 195.2
IOU avg: 0.433778

###
//-----------------------------------------------------------------------------------------//
###  - 7 -
13-02-22

"""
model.compile(optimizer = keras.optimizers.Adam(), 
			loss = pixelwise_binary_cross_entropy_tf, 
			eval_loss='binary_crossentropy',
            metrics = ['accuracy'])
"""

binarización con > 128 (umbral=128)
11 epochs
N (total samples) = 1500
train%samples 0.7
batch size = 1
Val_size: 450 samples

IOU total: 285.4
IOU avg: 0.634222

###
//-----------------------------------------------------------------------------------------//
###  - 8 -
13-02-22

"""
model.compile(optimizer = keras.optimizers.Adam(), 
			loss = pixelwise_binary_cross_entropy_tf, 
			eval_loss='binary_crossentropy',
            metrics = ['accuracy'])
"""

binarización con > 128 (umbral=128)
11 epochs
N (total samples) = 1500
train%samples 0.7
batch size = 1
Val_size: 450 samples

IOU total: 269.95
IOU avg: 0.599889

###
//-----------------------------------------------------------------------------------------//
###  - 9 -
14-02-22

"""
model.compile(optimizer = keras.optimizers.Adam(), 
			loss = pixelwise_binary_cross_entropy_tf, 
			eval_loss='binary_crossentropy',
            metrics = ['accuracy'])
"""

binarización con > 128 (umbral=128)
15 epochs
N (total samples) = 1500
train%samples 0.7
batch size = 1
Val_size: 450 samples

IOU total: 359.38
IOU avg: 0.798633

Nota: peso del bakground: 1
	  peso de las manos: 1

###
//-----------------------------------------------------------------------------------------//
###  - 10 -
15-02-22

"""
model.compile(optimizer = keras.optimizers.Adam(), 
			loss = pixelwise_binary_cross_entropy_tf, 
			eval_loss='binary_crossentropy',
            metrics = ['accuracy'])
"""

binarización con > 128 (umbral=128)
15 epochs
N (total samples) = 1500
train%samples 0.7
batch size = 2
Val_size: 450 samples

IOU total: 300.45
IOU avg: 0.687666

Nota: peso del bakground: 0.85
	  peso de las manos: 1
###
//-----------------------------------------------------------------------------------------//
###  - 11 -
15-02-22

"""
model.compile(optimizer = keras.optimizers.Adam(), 
			loss = pixelwise_binary_cross_entropy_tf, 
			eval_loss='binary_crossentropy',
            metrics = ['accuracy'])
"""

binarización con > 128 (umbral=128)
15 epochs
N (total samples) = 1500
train%samples 0.7
batch size = 2
Val_size: 150 samples

IOU total: 118.86
IOU avg: 0.7924

Nota: peso del bakground: 0.95
	  peso de las manos: 1
###
//-----------------------------------------------------------------------------------------//
###  - 12 -
15-02-22

"""
model.compile(optimizer = keras.optimizers.Adam(), 
			loss = pixelwise_binary_cross_entropy_tf, 
			eval_loss='binary_crossentropy',
            metrics = ['accuracy'])
"""

binarización con > 128 (umbral=128)
15 epochs
N (total samples) = 1750
train%samples 0.7
batch size = 2
Val_size: 175 samples

IOU total: 137.679
IOU avg: 0.786737

Nota: peso del bakground: 0.95
	  peso de las manos: 1
###
//-----------------------------------------------------------------------------------------//
###  - 13 -
21-02-22

"""
model.compile(optimizer = keras.optimizers.Adam(), 
			loss = pixelwise_binary_cross_entropy_tf, 
			eval_loss='binary_crossentropy',
            metrics = ['accuracy'])
"""

binarización con > 128 (umbral=128)
15 epochs
N (total samples) = 1750
train%samples 0.7
batch size = 2
Val_size: 175 samples

IOU total: 144.30356
IOU avg: 0.824592


Nota: 
      Manos separadas mediante: dilatación de uniones
      peso del bakground: 0.95 + mapa de pesos
	  peso de las manos: 1
###
//-----------------------------------------------------------------------------------------//
###  - 14 -
27-02-22

"""
model.compile(optimizer = keras.optimizers.Adam(), 
			loss = pixelwise_dice_loss_tf, 
			eval_loss='binary_crossentropy',
            metrics = ['accuracy'])
"""

binarización con > 128 (umbral=128)
15 epochs
N (total samples) = 1750
train%samples 0.7
batch size = 2
Val_size: 175 samples

IOU total: 127.791378
IOU avg: 0.730236


Nota: 
      Manos separadas mediante: dilatación de uniones
      peso del bakground: 0.95 + mapa de pesos
	  peso de las manos: 1
###
//-----------------------------------------------------------------------------------------//
###  - 15 -
27-02-22

"""
model.compile(optimizer = keras.optimizers.Adam(), 
			loss = pixelwise_dice_loss_tf, 
			eval_loss='binary_crossentropy',
            metrics = ['accuracy'])
"""

binarización con > 128 (umbral=128)
15 epochs
N (total samples) = 1750
train%samples 0.7
batch size = 2
Val_size: 175 samples

IOU total: 138.3326
IOU avg: 0.790472


Nota: 
      Manos separadas mediante: dilatación de uniones
      peso del bakground: 0.95 + mapa de pesos
	  peso de las manos: 1
###
//-----------------------------------------------------------------------------------------//
###  - 16 -
28-02-22

"""
model.compile(optimizer = keras.optimizers.Adam(), 
			loss = pixelwise_dice_loss_tf, 
			eval_loss='binary_crossentropy',
            metrics = ['accuracy'])
"""

binarización con > 128 (umbral=128)
25 epochs
N (total samples) = 1750
train%samples 0.7
batch size = 2
Val_size: 175 samples

IOU total: 142.666528
IOU avg: 0.815237


Nota: 
      Manos separadas mediante: dilatación de uniones
      peso del bakground: 0.95 + mapa de pesos
	  peso de las manos: 1
###
//-----------------------------------------------------------------------------------------//
###  - 17 -
28-02-22

"""
model.compile(optimizer = keras.optimizers.Adam(), 
			loss = pixelwise_dice_loss_tf, 
			eval_loss='binary_crossentropy',
            metrics = ['accuracy'])
"""

binarización con > 128 (umbral=128)
25 epochs
N (total samples) = 1750
train%samples 0.7
batch size = 2
Val_size: 175 samples
dropout = 0.05

IOU total: 134.35
IOU avg: 0.767714


Nota: 
      Malos resutados en general
###
//-----------------------------------------------------------------------------------------//
###  - 18 -
05-03-22

"""
model.compile(optimizer = keras.optimizers.SGD(lr=0.01, decay = 1e-6, momentum=0.9, clipnorm=1.0, nesterov = True), 
			loss = pixelwise_focal_loss_tf, alpha = 0.25, gamma = 2. 
			eval_loss='binary_crossentropy',
            metrics = ['accuracy'])
"""

binarización con > 128 (umbral=128)
20 epochs
N (total samples) = 1750
train%samples 0.7
batch size = 2
Val_size: 175 samples
dropout = 0.05

Nota: 
      Malos resutados
//-----------------------------------------------------------------------------------------//
###  - 19 -
06-03-22

"""
model.compile(optimizer = keras.optimizers.SGD(lr=0.01, decay = 1e-6, momentum=0.9, clipnorm=1.0, nesterov = True), 
			compiled_loss = pixelwise_focal_loss_tf, alpha = 0.5, gamma = 2. 
			eval_loss='binary_crossentropy',
            metrics = ['accuracy'])
"""

binarización con > 128 (umbral=128)
20 epochs
N (total samples) = 1900
train%samples 0.7
batch size = 2
Val_size: 190 samples
dropout = 0

IOU total: 127.307
IOU avg: 0.670

Nota: 
      Malos resutados
	  Train accuracy peaked at ~0.985 
//-----------------------------------------------------------------------------------------//
###  - 20 -
06-03-22

"""
model.compile(tf.keras.optimizers.SGD(lr=0.01, decay = 1e-6, momentum=0.9, clipnorm=1.0, nesterov = True), 
				copiled_loss = pixelwise_focal_loss_tf, alpha = 0.4, gamma = 1.5 
                loss = 'binary_crossentropy',
                metrics = ['accuracy'])
"""

binarización con > 128 (umbral=128)
20 epochs
N (total samples) = 1900
train%samples 0.7
batch size = 2
Val_size: 190 samples
dropout = 0

IOU total: 140.183
IOU avg: 0.7378

Nota: 
      Resultados muy pobres, bordes muy poco definidos
	  No se ha conseguido la separacion entre manos diferentes en general
	  Train accuracy ended at 0.9883
	  Val accuracy ended at 0.9817
//-----------------------------------------------------------------------------------------//
###  - 21 -
06-03-22

"""
model.compile(tf.keras.optimizers.SGD(lr=0.01, decay = 1e-6, momentum=0.9, clipnorm=1.0, nesterov = True), 
				copiled_loss = pixelwise_binary_cross_entropy_tf
                loss = 'binary_crossentropy',
                metrics = ['accuracy'])
"""

binarización con > 128 (umbral=128)
20 epochs
N (total samples) = 2000
train%samples 0.7
batch size = 2
Val_size: 200 samples
dropout = 0

IOU total: 157.6
IOU avg: 0.788

Nota: 
      Resultados malos en muchos casos
//-----------------------------------------------------------------------------------------//
###  - 22 -
06-03-22

"""
model.compile(tf.keras.optimizers.Adam(), 
				copiled_loss = pixelwise_binary_cross_entropy_tf
                loss = 'binary_crossentropy',
                metrics = ['accuracy'])
"""

binarización con > 128 (umbral=128)
25 epochs
N (total samples) = 2000
train%samples 0.7
batch size = 2
Val_size: 200 samples
dropout = 0

IOU total: 157.6
IOU avg: 0.788

Nota: 
	  Peso BG base: 0.95
	  Peso F: 1
      Resultados malos en muchos casos
      He intentado replicar #13, que produjo muy buenos resultados y no lo he conseguido
	  la unica diferencia que veo es que ahora utilizo 25 epochs. Si es un problema de 
	  overfitting se invalidarian todas mis pruebas anteriores con dice loss y con focal loss
//-----------------------------------------------------------------------------------------//
###  - 23 -
06-03-22

"""
model.compile(tf.keras.optimizers.Adam(), 
				copiled_loss = pixelwise_binary_cross_entropy_tf
                loss = 'binary_crossentropy',
                metrics = ['accuracy'])
"""

binarización con > 128 (umbral=128)
12 epochs
N (total samples) = 2000
train%samples 0.7
batch size = 2
Val_size: 200 samples
dropout = 0

IOU total: 162.154
IOU avg: 0.8107

Nota: 
	  Peso BG base: 0.95
	  Peso F: 1
      Buenos resultados, puede que hayan faltado unas epochs
	  Si parece que fuese un problema de overfitting, tendre que repetir todas las pruebas
//-----------------------------------------------------------------------------------------//
###  - 24 -
07-03-22

"""
model.compile(tf.keras.optimizers.SGD(lr=0.01, decay = 1e-6, momentum=0.9, clipnorm=1.0, nesterov = True),
				copiled_loss = pixelwise_binary_cross_entropy_tf
                loss = 'binary_crossentropy',
                metrics = ['accuracy'])
"""

binarización con > 128 (umbral=128)
12 epochs
N (total samples) = 2000
train%samples 0.7
batch size = 2
Val_size: 200 samples
dropout = 0

IOU total: 160.88
IOU avg: 0.8044

Nota: 
	  Peso BG base: 0.95
	  Peso FG: 1
      Buenos resultados, puede que hayan faltado unas epochs
	  Resultados ligeramente peores a los de Adam
//-----------------------------------------------------------------------------------------//
###  - 25 -
08-03-22

"""
model.compile(tf.keras.optimizers.SGD(lr=0.01, decay = 1e-6, momentum=0.9, clipnorm=1.0, nesterov = True),
				copiled_loss = pixelwise_binary_cross_entropy_tf
                loss = 'binary_crossentropy',
                metrics = ['accuracy'])
"""

binarización con > 128 (umbral=128)
12 epochs
N (total samples) = 2200
train%samples 0.7
batch size = 2
Val_size: 220 samples
dropout = 0

IOU total: 170.27
IOU avg: 0.7739

Nota: 
	  Peso BG base: 0.95
	  Peso FG: 1
      Buenos resultados, puede que hayan faltado unas epochs
	  Resultados ligeramente peores a los de Adam
	  Mapas de pesos exactos
//-----------------------------------------------------------------------------------------//
###  - 26 -
09-03-22

"""
model.compile(tf.keras.optimizers.SGD(lr=0.01, decay = 1e-6, momentum=0.9, clipnorm=1.0, nesterov = True),
				copiled_loss = pixelwise_binary_cross_entropy_tf
                loss = 'binary_crossentropy',
                metrics = ['accuracy'])
"""

binarización con > 128 (umbral=128)
6 epochs
N (total samples) = 4800
train%samples 0.75
batch size = 4
Val_size: 240 samples
dropout = 0

IOU total: 189.1498
IOU avg: 0.78812

Nota: 
	  Peso BG base: 0.95
	  Peso FG: 1
      Buenos resultados, puede que hayan faltado unas epochs
	  Resultados ligeramente peores a los de Adam
	  Mapas de pesos exactos
//-----------------------------------------------------------------------------------------//
###  - 27 -
09-03-22

"""
model.compile(tf.keras.optimizers.SGD(lr=0.01, decay = 1e-6, momentum=0.9, clipnorm=1.0, nesterov = True),
				copiled_loss = pixelwise_binary_cross_entropy_tf
                loss = 'binary_crossentropy',
                metrics = ['accuracy'])
"""

binarización con > 128 (umbral=128)
12 epochs
N (total samples) = 4800
train%samples 0.75
batch size = 4
Val_size: 240 samples
dropout = 0

IOU total: 190.722
IOU avg: 0.7946

Nota: 
	  Peso BG base: 0.95
	  Peso FG: 1
      Buenos resultados, puede que hayan sobrado unas epochs
	  Mapas de pesos exactos
//-----------------------------------------------------------------------------------------//
###  - 28 -
10-03-22

"""
///////////New model test0///////////////////
model.compile(tf.keras.optimizers.SGD(lr=0.01, decay = 1e-6, momentum=0.9, clipnorm=1.0, nesterov = True),
				copiled_loss = pixelwise_binary_cross_entropy_tf
                loss = 'binary_crossentropy',
                metrics = ['accuracy'])
"""

binarización con > 128 (umbral=128)
15 epochs
N (total samples) = 4800
train%samples 0.75
batch size = 4
Val_size: 240 samples
dropout = 0.5

IOU total: 127.7542
IOU avg: 0.5323

Nota: 
	  Peso BG base: 0.95
	  Peso FG: 1
      Malos resultados
	  Ejecucion muy lenta
	  Puede tener potencia, pero con otro tipo de entrenamiento mucho mas largo
//-----------------------------------------------------------------------------------------//
###  - 29 -
10-03-22

"""
model.compile(tf.keras.optimizers.Adam(),
				copiled_loss = weighted_dice_loss_tf
                loss = 'binary_crossentropy',
                metrics = ['accuracy'])
"""

binarización con > 128 (umbral=128)
12 epochs (early stopping)
N (total samples) = 4800
train%samples 0.75
batch size = 4
Val_size: 240 samples
dropout = 0.5

IOU total: 177.2547
IOU avg: 0.7385

Nota: 
	  Peso BG base: 0.95
	  Peso FG: 1
      Buenos resultados en algunos casos pero demasiados falsos positivos
	  Bastante peor que cross entropy
//-----------------------------------------------------------------------------------------//
###  - 30 -
12-03-22

"""
model.compile(tf.keras.optimizers.Adam(),
				copiled_loss = weighted_pixelsise_focal_loss_tf, alpha = 0.4, gamma = 1.5
                loss = 'binary_crossentropy',
                metrics = ['accuracy'])
"""

binarización con > 128 (umbral=128)
12 epochs
N (total samples) = 4800
train%samples 0.75
batch size = 4
Val_size: 240 samples
dropout = 0.5

IOU total: 176.95833
IOU avg: 0.7373

Nota: 
	  Peso BG base: 0.95
	  Peso FG: 1
      Demasiados falsos positivos
	  Bastante peor que cross entropy
//-----------------------------------------------------------------------------------------//
###  - 31 -
12-03-22

"""
model.compile(tf.keras.optimizers.Adam(),
				copiled_loss = weighted_pixelsise_binary_crossentropy_tf
                loss = 'binary_crossentropy',
                metrics = ['accuracy'])
"""

binarización con > 128 (umbral=128)
12 epochs
N (total samples) = 4800
train%samples 0.75
batch size = 4
Val_size: 240 samples
dropout = 0.5

IOU total: 178.9585
IOU avg: 0.74566

Nota: 
	  Peso BG base: 0.95
	  Peso FG: 1
      Demasiados falsos positivos
	  Concluyo asi que blobs de falsos positivos se deben al dropout y por lo tanto seguire entrenando sin dropout
//-----------------------------------------------------------------------------------------//
###  - 32 -
12-03-22

"""
model.compile(tf.keras.optimizers.Adam(),
				copiled_loss = weighted_dice_loss
                loss = 'binary_crossentropy',
                metrics = ['accuracy'])
"""

binarización con > 128 (umbral=128)
12 epochs
N (total samples) = 4800
train%samples 0.75
batch size = 4
Val_size: 240 samples

IOU total: 190.7848
IOU avg: 0.7949

Nota: 
	  Peso BG base: 0.95
	  Peso FG: 1
//-----------------------------------------------------------------------------------------//
###  - 33 -
12-03-22

"""
model.compile(tf.keras.optimizers.SGD(
                        lr = 0.01,
                        momentum=0.9,
                        decay = 1e-6,
                        clipnorm=1.0, 
                        nesterov = False),
				copiled_loss = weighted_pixelwise_focal_loss_tf alpha = 0.526, gamma = 1.5
                loss = 'binary_crossentropy',
                metrics = ['accuracy'])
"""

binarización con > 128 (umbral=128)
12 epochs
N (total samples) = 4800
train%samples 0.75
batch size = 4
Val_size: 240 samples

IOU total: 174.196
IOU avg: 0.7258

Nota: 
	  Peso BG base: 0.95
	  Peso FG: 1
//-----------------------------------------------------------------------------------------//
###  - 34 -
13-03-22

"""
optimizer = keras.optimizers.SGD(
                        lr = 0.01,
                        momentum=0.9,
                        decay = 1e-4,
                        clipnorm=1.0, 
                        nesterov = False)
				copiled_loss = weighted_pixelwise_crossentropy_tf
                loss = 'binary_crossentropy',
                metrics = ['accuracy'])
"""

binarización con > 128 (umbral=128)
12 epochs
N (total samples) = 4800
train%samples 0.75
batch size = 4
Val_size: 240 samples

IOU total: 193.7582
IOU avg: 0.80732

Nota: 
	  Peso BG base: 0.95
	  Peso FG: 1
	  Modelo ligeramente aumentado :
		 Convolucion de entrada a (5, 5)
		 Convoluciones en la entrada no separables
		 4M de parametros
//-----------------------------------------------------------------------------------------//
###  - 35 -
13-03-22

"""
optimizer = keras.optimizers.SGD(
                        lr = 0.02,
                        momentum=0.9,
                        decay = 1e-3,
                        clipnorm=1.0, 
                        nesterov = True)
				copiled_loss = weighted_pixelwise_crossentropy_tf
                loss = 'binary_crossentropy',
                metrics = ['accuracy'])
"""

binarización con > 128 (umbral=128)
12 epochs
N (total samples) = 4800
train%samples 0.75
batch size = 4
Val_size: 240 samples

IOU total: 183.002
IOU avg: 0.7625

Nota: 
	  Peso BG base: 0.95
	  Peso FG: 1
	  Modelo ligeramente aumentado :
		 Convolucion de entrada a (5, 5)
		 Convoluciones en el medio no separables y 256 canales en vez de 512
		 4.2M de parametros
//-----------------------------------------------------------------------------------------//
###  - 36 -
15-03-22

"""
optimizer = keras.optimizers.Adam()
				copiled_loss = weighted_pixelwise_crossentropy_tf
                loss = 'binary_crossentropy',
                metrics = ['accuracy'])
"""

binarización con > 128 (umbral=128)
12 epochs
N (total samples) = 4800
train_samples = 3600
batch size = 2
Val_size: 300 samples

IOU total: 231.54
IOU avg: 0.7718

Nota: 
	  Peso BG base: 0.95
	  Peso FG: 1
	  Convoluciones no separables
	  Vides diferentes para entrenar validar y testear
	  Buenos resultados
//-----------------------------------------------------------------------------------------//
###  - 37 -
15-03-22

"""
optimizer = keras.optimizers.Adam()
				copiled_loss = weighted_pixelwise_crossentropy_tf
                loss = 'binary_crossentropy',
                metrics = ['accuracy'])
"""

binarización con > 128 (umbral=128)
12 epochs
N (total samples) = 4800
train_samples = 3600
batch size = 2
Val_size: 300 samples

IOU total: 225.17
IOU avg: 0.7506

Nota: 
	  Peso BG base: 0.95
	  Peso FG: 1
	  Convoluciones no separables
	  Capa intermedia con convoluciones separables
	  Vides diferentes para entrenar validar y testear
	  Peores resultados
//-----------------------------------------------------------------------------------------//
###  - 38 -
15-03-22

"""
optimizer = keras.optimizers.Adam()
				copiled_loss = weighted_pixelwise_crossentropy_tf
                loss = 'binary_crossentropy',
                metrics = ['accuracy'])
"""

binarización con > 128 (umbral=128)
12 epochs
N (total samples) = 4800
train_samples = 3600
batch size = 2
Val_size: 300 samples

IOU total: 224.929
IOU avg: 0.7497

Nota: 
	  Peso BG base: 0.95
	  Peso FG: 1
	  Convoluciones no separables
	  Kernels (5,5)
//-----------------------------------------------------------------------------------------//
###  - 39 -
15-03-22

"""
optimizer = keras.optimizers.Adam()
				copiled_loss = weighted_pixelwise_crossentropy_tf
                loss = 'binary_crossentropy',
                metrics = ['accuracy'])
"""

binarización con > 128 (umbral=128)
12 epochs
N (total samples) = 4800
train_samples = 3600
batch size = 2
Val_size: 300 samples
dropout 0.2

IOU total: 225.7
IOU avg: 0.7523

Nota: 
	  Peso BG base: 0.95
	  Peso FG: 1
	  Convoluciones no separables
	  Kernels (3,3)
//-----------------------------------------------------------------------------------------//
###  - 40 -
15-03-22

"""
optimizer = keras.optimizers.Adam()
				copiled_loss = weighted_pixelwise_crossentropy_tf
                loss = 'binary_crossentropy',
                metrics = ['accuracy'])
"""

binarización con > 128 (umbral=128)
20 epochs
N (total samples) = 4800
train_samples = 3600
batch size = 2
Val_size: 300 samples
dropout 0.2 encoder

IOU total: 220.17
IOU avg: 0.7339

Nota: 
	  Peso BG base: 0.95
	  Peso FG: 1
	  Convoluciones no separables
	  Kernels (3,3)
//-----------------------------------------------------------------------------------------//
###  - 41 -
25-03-22

"""
optimizer = keras.optimizers.Adam()
				copiled_loss = weighted_pixelwise_crossentropy_tf
                loss = 'binary_crossentropy',
                metrics = ['accuracy'])
"""

binarización con > 128 (umbral=128)
30 epochs
N (total samples) = 4800
train_samples = 3600
batch size = 2
Val_size: 300 samples
dropout 0.4

IOU total: 163.87
IOU avg: 0.5629

Nota: 
	  Peso BG base: 0.95
	  Peso FG: 1
	  Convoluciones no separables
	  Kernels (3,3)
//-----------------------------------------------------------------------------------------//

###  - 42 -
25-03-22

"""
optimizer = keras.optimizers.Adam()
				copiled_loss = weighted_pixelwise_crossentropy_tf
                loss = 'binary_crossentropy',
                metrics = ['accuracy'])
"""

binarización con > 128 (umbral=128)
25 epochs
N (total samples) = 4800
train_samples = 3600
batch size = 2
Val_size: 300 samples
dropout 0.2

IOU total: 241.761
IOU avg: 0.80587

Nota: 
	  Peso BG base: 0.95
	  Peso FG: 1
	  Convoluciones no separables
	  Kernels (3,3)
//-----------------------------------------------------------------------------------------//
###  - 43 -
26-03-22

"""
optimizer = keras.optimizers.Adam()
				copiled_loss = weighted_pixelwise_crossentropy_tf
                loss = 'binary_crossentropy',
                metrics = ['accuracy'])
"""

binarización con > 128 (umbral=128)
30 epochs
N (total samples) = 4800
train_samples = 3600
batch size = 2
test_size: 500 samples
dropout 0.25

IOU total: 241.761
IOU avg: 0.78773

Nota: 
	  Peso BG base: 0.95
	  Peso FG: 1
	  Convoluciones no separables
	  Kernels (3,3)
//-----------------------------------------------------------------------------------------//

//------------------------------------------------------------------------------------------//
//-----------------------------------------------------------------------------------------//
###  - 01 -
24-04-22
Model0_BCE_05

"""
optimizer = keras.optimizers.Adam()
				copiled_loss = weighted_pixelwise_crossentropy_tf
                loss = 'binary_crossentropy',
                metrics = ['accuracy'])
"""

binarización con > 128 (umbral=128)
30 epochs
N (total samples) = 4800
train_samples = 3600
batch size = 2
test_size: 500 samples
dropout 0.5

IOU total: 370.74
IOU avg: 0.7414

Nota: 
	  Peso BG base: 0.95
	  Peso FG: 1
	  Convoluciones no separables
	  Kernels (3,3)
//-----------------------------------------------------------------------------------------//
###  - 02 -
24-04-22
Model0_BCE_06

"""
optimizer = keras.optimizers.Adam()
				copiled_loss = weighted_pixelwise_crossentropy_tf
                loss = 'binary_crossentropy',
                metrics = ['accuracy'])
"""

binarización con > 128 (umbral=128)
30 epochs
N (total samples) = 4800
train_samples = 3600
batch size = 2
test_size: 500 samples
dropout 0.6

IOU total: 395.78
IOU avg: 0.791559

Nota: 
	  Peso BG base: 0.95
	  Peso FG: 1
	  Convoluciones no separables
	  Kernels (3,3)
	  Admitiría más epochs
//-----------------------------------------------------------------------------------------//
###  - 03 -
25-04-22
UNET_BCE_04

"""
optimizer = keras.optimizers.Adam()
				copiled_loss = weighted_pixelwise_crossentropy_tf
                loss = 'binary_crossentropy',
                metrics = ['accuracy'])
"""

binarización con > 128 (umbral=128)
30 epochs
N (total samples) = 4800
train_samples = 3600
batch size = 2
test_size: 500 samples
dropout 0.4

IOU total: 334.98
IOU avg: 0.66996

Nota: 
	  Peso BG base: 0.95
	  Peso FG: 1
	  Convoluciones no separables
	  Kernels (3,3)
	  Admitiría muchas más epochs
//-----------------------------------------------------------------------------------------//
###  - 04 -
25-04-22
model0_BCE_04

"""
optimizer = keras.optimizers.Adam()
				copiled_loss = weighted_pixelwise_crossentropy_tf
                loss = 'binary_crossentropy',
                metrics = ['accuracy'])
"""

binarización con > 128 (umbral=128)
30 epochs
N (total samples) = 4800
train_samples = 3600
batch size = 2
test_size: 500 samples
dropout 0.6

IOU total: 396.58
IOU avg: 0.7931

Nota: 
	  Peso BG base: 0.95
	  Peso FG: 1
	  Convoluciones no separables
	  Kernels (3,3)
//-----------------------------------------------------------------------------------------//
###  - 05 -
25-04-22
UNET_BCE_04

"""
optimizer = keras.optimizers.Adam()
				copiled_loss = weighted_pixelwise_crossentropy_tf
                loss = 'binary_crossentropy',
                metrics = ['accuracy'])
"""

binarización con > 128 (umbral=128)
30 epochs
N (total samples) = 4800
train_samples = 3600
batch size = 2
test_size: 500 samples
dropout 0.4

IOU total: 334.98
IOU avg: 0.66996

training time = 4.041h

Nota: 
	  Peso BG base: 0.95
	  Peso FG: 1
	  Convoluciones no separables
	  Kernels (3,3)
	  Admitiría muchas más epochs
//-----------------------------------------------------------------------------------------//
###  - 06 -
25-04-22
model0_BCE_03

"""
optimizer = keras.optimizers.Adam()
				copiled_loss = weighted_pixelwise_crossentropy_tf
                loss = 'binary_crossentropy',
                metrics = ['accuracy'])
"""

binarización con > 128 (umbral=128)
30 epochs
N (total samples) = 4800
train_samples = 3600
batch size = 2
test_size: 500 samples
dropout 0.3

IOU total: 384.9473
IOU avg: 0.7699

training time = 4.110h

Nota: 
	  Peso BG base: 0.95
	  Peso FG: 1
	  Convoluciones no separables
	  Kernels (3,3)
//-----------------------------------------------------------------------------------------//
###  - 07 - MODELO REPTIDO PORQUE SOY GILIPOLLAS
1-05-22
model0_BCE_03

"""
optimizer = keras.optimizers.Adam()
				copiled_loss = weighted_pixelwise_crossentropy_tf
                loss = 'binary_crossentropy',
                metrics = ['accuracy'])
"""

binarización con > 128 (umbral=128)
30 epochs
N (total samples) = 4800
train_samples = 3600
batch size = 2
test_size: 500 samples
dropout 0.3

IOU total: 400.516
IOU avg: 0.8010

training time = 4.155h

Nota: 
	  Peso BG base: 0.95
	  Peso FG: 1
	  Convoluciones no separables
	  Kernels (3,3)
//-----------------------------------------------------------------------------------------//
###  - 08 -
1-05-22
model0_BCE_02

"""
optimizer = keras.optimizers.Adam()
				copiled_loss = weighted_pixelwise_crossentropy_tf
                loss = 'binary_crossentropy',
                metrics = ['accuracy'])
"""

binarización con > 128 (umbral=128)
30 epochs
N (total samples) = 4800
train_samples = 3600
batch size = 2
test_size: 500 samples
dropout 0.2

IOU total: 377.7056
IOU avg: 0.7554

training time = 4.090h

Nota: 
	  Peso BG base: 0.95
	  Peso FG: 1
	  Convoluciones no separables
	  Kernels (3,3)
//-----------------------------------------------------------------------------------------//
###  - 09 -
1-05-22
model_PReLU_BCE_07

"""
optimizer = keras.optimizers.Adam()
				copiled_loss = weighted_pixelwise_crossentropy_tf
                loss = 'binary_crossentropy',
                metrics = [binary_io_u])
"""

binarización con > 128 (umbral=128)
30 epochs
N (total samples) = 4800
train_samples = 3600
batch size = 2
test_size: 500 samples
dropout 0.7

IOU total: ???
IOU avg: ~0.7 / ???

training time = 4.20h

Nota: 
	  Peso BG base: 0.95
	  Peso FG: 1
	  Convoluciones no separables
	  Kernels (3,3)
//-----------------------------------------------------------------------------------------//
###  - 10 -
2-05-22
model0_BCE_00

"""
optimizer = keras.optimizers.Adam()
				copiled_loss = weighted_pixelwise_crossentropy_tf
                loss = 'binary_crossentropy',
                metrics = [binary_io_u])
"""

binarización con > 128 (umbral=128)
30 epochs
N (total samples) = 4800
train_samples = 3600
batch size = 2
test_size: 500 samples
dropout 0

IOU total: 399.91
IOU avg: 0.79982

training time = 3.89h

Nota: 
	  Peso BG base: 0.95
	  Peso FG: 1
	  Convoluciones no separables
	  Kernels (3,3)
//-----------------------------------------------------------------------------------------//
###  - 11 -
2-05-22
model0_BCE_01

"""
optimizer = keras.optimizers.Adam()
				copiled_loss = weighted_pixelwise_crossentropy_tf
                loss = 'binary_crossentropy',
                metrics = [binary_io_u])
"""

binarización con > 128 (umbral=128)
30 epochs
N (total samples) = 4800
train_samples = 3600
batch size = 2
test_size: 500 samples
dropout 0.1

IOU total: 382.089
IOU avg: 0.7642

training time = 4.1h

Nota: 
	  Peso BG base: 0.95
	  Peso FG: 1
	  Convoluciones no separables
	  Kernels (3,3)
//-----------------------------------------------------------------------------------------//
###  - 12 -
2-05-22
model0_BCE_02

"""
optimizer = keras.optimizers.Adam()
				copiled_loss = weighted_pixelwise_crossentropy_tf
                loss = 'binary_crossentropy',
                metrics = [binary_io_u])
"""

binarización con > 128 (umbral=128)
30 epochs
N (total samples) = 4800
train_samples = 3600
batch size = 2
test_size: 500 samples
dropout 0.2

IOU total: 404.2343
IOU avg: 0.8085
training time = 3.83h

Nota: 
	  Peso BG base: 0.95
	  Peso FG: 1
	  Convoluciones no separables
	  Kernels (3,3)
	  Muy top
//-----------------------------------------------------------------------------------------//
###  - 13 -
2-05-22
model0_BCE_03

"""
optimizer = keras.optimizers.Adam()
				copiled_loss = weighted_pixelwise_crossentropy_tf
                loss = 'binary_crossentropy',
                metrics = [binary_io_u])
"""

binarización con > 128 (umbral=128)
30 epochs
N (total samples) = 4800
train_samples = 3600
batch size = 2
test_size: 500 samples
dropout 0.3

IOU total: 407.1213
IOU avg: 0.81424
training time = 3.83h

Nota: 
	  Peor definición
//-----------------------------------------------------------------------------------------//
###  - 14 -
3-05-22
model0_BCE_04

"""
optimizer = keras.optimizers.Adam()
				copiled_loss = weighted_pixelwise_crossentropy_tf
                loss = 'binary_crossentropy',
                metrics = [binary_io_u])
"""

binarización con > 128 (umbral=128)
30 epochs
N (total samples) = 4800
train_samples = 3600
batch size = 2
test_size: 500 samples
dropout 0.4

IOU total: 356.682
IOU avg: 0.7154
training time = 4.1h

Nota: 
	  Resultados muy pobres
//-----------------------------------------------------------------------------------------//
###  - 15 -
4-05-22
model0_BCE_05

"""
optimizer = keras.optimizers.Adam()
				copiled_loss = weighted_pixelwise_crossentropy_tf
                loss = 'binary_crossentropy',
                metrics = [binary_io_u])
"""

binarización con > 128 (umbral=128)
30 epochs
N (total samples) = 4800
train_samples = 3600
batch size = 2
test_size: 500 samples
dropout 0.5

IOU total: 392.224
IOU avg: 0.7844
training time = 3.83h

Nota: 
	  Resultados muy pobres
//-----------------------------------------------------------------------------------------//
###  - 16 -
4-05-22
model0_BCE_06

"""
optimizer = keras.optimizers.Adam()
				copiled_loss = weighted_pixelwise_crossentropy_tf
                loss = 'binary_crossentropy',
                metrics = [binary_io_u])
"""

binarización con > 128 (umbral=128)
30 epochs
N (total samples) = 4800
train_samples = 3600
batch size = 2
test_size: 500 samples
dropout 0.6

IOU total: 383.4877
IOU avg: 0.7669
training time = 3.83h

Nota: 
	  Resultados pobres
//-----------------------------------------------------------------------------------------//
###  - 16 -
4-05-22
model0_BCE_07

"""
optimizer = keras.optimizers.Adam()
				copiled_loss = weighted_pixelwise_crossentropy_tf
                loss = 'binary_crossentropy',
                metrics = [binary_io_u])
"""

binarización con > 128 (umbral=128)
30 epochs
N (total samples) = 4800
train_samples = 3600
batch size = 2
test_size: 500 samples
dropout 0.7

IOU total: 364.7687
IOU avg: 0.7295
training time = 3.83h

Nota: 
	  Esta bastante bien la vd
//-----------------------------------------------------------------------------------------//
###  - 17 -
6-05-22
UNET3_BCE_00

"""
optimizer = keras.optimizers.Adam()
				copiled_loss = weighted_pixelwise_crossentropy_tf
                loss = 'binary_crossentropy',
                metrics = [binary_io_u])
"""

binarización con > 128 (umbral=128)
30 epochs
N (total samples) = 4800
train_samples = 3600
batch size = 2
test_size: 500 samples
dropout 0.0

IOU total: 388.53
IOU avg: 0.777
training time = 5.28h

Nota: 
	  
//-----------------------------------------------------------------------------------------//
###  - 18 -
6-05-22
UNET1_BCE_00

"""
optimizer = keras.optimizers.Adam()
				copiled_loss = weighted_pixelwise_crossentropy_tf
                loss = 'binary_crossentropy',
                metrics = [binary_io_u])
"""

binarización con > 128 (umbral=128)
30 epochs
N (total samples) = 4800
train_samples = 3600
batch size = 2
test_size: 500 samples
dropout 0.0

IOU total: 384.34
IOU avg: 0.7686
training time = 5.27h

Nota: 
	  
//-----------------------------------------------------------------------------------------//
###  - 18 -
6-05-22
model1_BCE_02

"""
optimizer = keras.optimizers.Adam()
				copiled_loss = weighted_pixelwise_crossentropy_tf
                loss = 'binary_crossentropy',
                metrics = [binary_io_u])
"""

binarización con > 128 (umbral=128)
30 epochs
N (total samples) = 4800
train_samples = 3600
batch size = 2
test_size: 500 samples
dropout 0.2

IOU total: 402.065
IOU avg: 0.8042
training time = 5.27h

Nota: 
	una convolución más en la entrada	  
//-----------------------------------------------------------------------------------------//
###  - 18 -
6-05-22
model2_BCE_02

"""
optimizer = keras.optimizers.Adam()
				copiled_loss = weighted_pixelwise_crossentropy_tf
                loss = 'binary_crossentropy',
                metrics = [binary_io_u])
"""

binarización con > 128 (umbral=128)
30 epochs
N (total samples) = 4800
train_samples = 3600
batch size = 2
test_size: 500 samples
dropout 0.2

IOU total: 366.15
IOU avg: 0.7323
training time = 5.27h

Nota:	
	He normal
	Una capa de profundidad menos y una convolución más en la entrada
	  
//-----------------------------------------------------------------------------------------//
###  - 19 -
6-05-22
UNET0_BCE_00

"""
optimizer = keras.optimizers.Adam()
				copiled_loss = weighted_pixelwise_crossentropy_tf
                loss = 'binary_crossentropy',
                metrics = [binary_io_u])
"""

binarización con > 128 (umbral=128)
30 epochs
N (total samples) = 4800
train_samples = 3600
batch size = 2
test_size: 500 samples
dropout 0.0

IOU total: 368.08
IOU avg: 0.7362
training time = 5.27h

Nota:	
	LeCun normal
	  
//-----------------------------------------------------------------------------------------//
###  - 19 -
6-05-22
UNET0_BCE_01

"""
optimizer = keras.optimizers.Adam()
				copiled_loss = weighted_pixelwise_crossentropy_tf
                loss = 'binary_crossentropy',
                metrics = [binary_io_u])
"""

binarización con > 128 (umbral=128)
30 epochs
N (total samples) = 4800
train_samples = 3600
batch size = 2
test_size: 500 samples
dropout 0.1

IOU total: ??
IOU avg: 0.76
training time = 5.27h

Nota:	
	LeCun normal
	  
//-----------------------------------------------------------------------------------------//
###  - 20 -
6-05-22
UNET0_BCE_02

"""
optimizer = keras.optimizers.Adam()
				copiled_loss = weighted_pixelwise_crossentropy_tf
                loss = 'binary_crossentropy',
                metrics = [binary_io_u])
"""

binarización con > 128 (umbral=128)
30 epochs
N (total samples) = 4800
train_samples = 3600
batch size = 2
test_size: 500 samples
dropout 0.2

IOU total: 354.26
IOU avg: 0.7085
training time = 5.15h

Nota:	
	LeCun normal
	  
//-----------------------------------------------------------------------------------------//
###  - 21 -
17-05-22
UNET0_BCE_03

"""
optimizer = keras.optimizers.Adam()
				copiled_loss = weighted_pixelwise_crossentropy_tf
                loss = 'binary_crossentropy',
                metrics = [binary_io_u])
"""

binarización con > 128 (umbral=128)
30 epochs
N (total samples) = 4800
train_samples = 3600
batch size = 2
test_size: 500 samples
dropout 0.3

IOU total: 372.30
IOU avg: 0.7446
training time = 5.15h

Nota:	
	LeCun normal
	  
//-----------------------------------------------------------------------------------------//
###  - 21 -
17-05-22
model0_WDL_02

"""
optimizer = keras.optimizers.Adam()
				copiled_loss = weighted_pixelwise_crossentropy_tf
                loss = 'binary_crossentropy',
                metrics = [binary_io_u])
"""

binarización con > 128 (umbral=128)
30 epochs
N (total samples) = 4800
train_samples = 3600
batch size = 2
test_size: 500 samples
dropout 0.2

IOU total: 394.17
IOU avg: 0.788
training time = 4.31h

Nota:	
	poca precisión
	k = 15 (Función de binarización continua)
	Creo que hace flata una k mayor
	  
//-----------------------------------------------------------------------------------------//
###  - 22 -
1-06-22
model0V2_aug_BCE_00_1

"""
optimizer = keras.optimizers.Adam()
				copiled_loss = weighted_pixelwise_crossentropy_tf
                loss = 'binary_crossentropy',
                metrics = [binary_io_u])
"""

binarización con > 128 (umbral=128)
30 epochs
N (total samples) = 4800
train_samples = 3600 + 1222
batch size = 2
test_size: 500 samples
dropout 0.0

IOU total: 396.075
IOU avg: 0.7922
training time = ??

Nota:	
	Data augmentation
	Muy buenos resultados
	  
//-----------------------------------------------------------------------------------------//
###  - 23 -
1-06-22
model0V2_aug_BCE_00_2

"""
optimizer = keras.optimizers.Adam()
				copiled_loss = weighted_pixelwise_crossentropy_tf
                loss = 'binary_crossentropy',
                metrics = [binary_io_u])
"""

binarización con > 128 (umbral=128)
30 epochs
N (total samples) = 4800
train_samples = 3600 + 1222
batch size = 2
test_size: 500 samples
dropout 0.0

IOU total: 398.56
IOU avg: 0.7971
training time = ??

Nota:	
	Data augmentation
	Buenos resultados
	  
//-----------------------------------------------------------------------------------------//
###  - 24 -
3-06-22
model0V2_aug_BCE_01_1

"""
optimizer = keras.optimizers.Adam()
				copiled_loss = weighted_pixelwise_crossentropy_tf
                loss = 'binary_crossentropy',
                metrics = [binary_io_u])
"""

binarización con > 128 (umbral=128)
30 epochs
N (total samples) = 4800
train_samples = 3600 + 1222
batch size = 2
test_size: 500 samples
dropout 0.1

IOU total: 400.98
IOU avg: 0.80196
training time = 5.47h

Nota:	
	Data augmentation
	Buenos resultados
	  
//-----------------------------------------------------------------------------------------//
###  - 25 -
6-06-22
model0V2_aug_BCE_01_2

"""
optimizer = keras.optimizers.Adam()
				copiled_loss = weighted_pixelwise_crossentropy_tf
                loss = 'binary_crossentropy',
                metrics = [binary_io_u])
"""

binarización con > 128 (umbral=128)
30 epochs
N (total samples) = 4800
train_samples = 3600 + 1222
batch size = 2
test_size: 500 samples
dropout 0.1

IOU total: 397.34
IOU avg: 0.7946
training time = 5.3h

Nota:	
	Data augmentation
	Buenos resultados
	  
//-----------------------------------------------------------------------------------------//
###  - 26 -
6-06-22
model0V2_aug_BCE_02_1

"""
optimizer = keras.optimizers.Adam()
				copiled_loss = weighted_pixelwise_crossentropy_tf
                loss = 'binary_crossentropy',
                metrics = [binary_io_u])
"""

binarización con > 128 (umbral=128)
30 epochs
N (total samples) = 4800
train_samples = 3600 + 1222
batch size = 2
test_size: 500 samples
dropout 0.2

IOU total: 398.54
IOU avg: 0.7971
training time = 5.3h

Nota:	
	Data augmentation
	Buenos resultados
	  
//-----------------------------------------------------------------------------------------//
###  - 26 -
6-06-22
model0V2_aug_BCE_02_2

"""
optimizer = keras.optimizers.Adam()
				copiled_loss = weighted_pixelwise_crossentropy_tf
                loss = 'binary_crossentropy',
                metrics = [binary_io_u])
"""

binarización con > 128 (umbral=128)
30 epochs
N (total samples) = 4800
train_samples = 3600 + 1222
batch size = 2
test_size: 500 samples
dropout 0.2

IOU total: 402.54
IOU avg: 0.8051
training time = 5.3h

Nota:	
	Data augmentation
	Buenos resultados
	  
//-----------------------------------------------------------------------------------------//
###  - 27 -
6-06-22
model0V2_aug_BCE_03_1

"""
optimizer = keras.optimizers.Adam()
				copiled_loss = weighted_pixelwise_crossentropy_tf
                loss = 'binary_crossentropy',
                metrics = [binary_io_u])
"""

binarización con > 128 (umbral=128)
30 epochs
N (total samples) = 4800
train_samples = 3600 + 1222
batch size = 2
test_size: 500 samples
dropout 0.3

IOU total: 404.8055
IOU avg: 0.8096
training time = 5.51h

Nota:	
	Data augmentation
	Buenos resultados
	  
//-----------------------------------------------------------------------------------------//
###  - 28 -
10-06-22
model0V2_aug_BCE_03_2

"""
optimizer = keras.optimizers.Adam()
				copiled_loss = weighted_pixelwise_crossentropy_tf
                loss = 'binary_crossentropy',
                metrics = [binary_io_u])
"""

binarización con > 128 (umbral=128)
30 epochs
N (total samples) = 4800
train_samples = 3600 + 1222
batch size = 2
test_size: 500 samples
dropout 0.3

IOU total: 404.58
IOU avg: 0.8092
training time = 5.51h

Nota:	
	Data augmentation
	Buenos resultados
	  
//-----------------------------------------------------------------------------------------//
### - 29 -
10-06-22
model0V2_aug_BCE_04_1

"""
optimizer = keras.optimizers.Adam()
				copiled_loss = weighted_pixelwise_crossentropy_tf
                loss = 'binary_crossentropy',
                metrics = [binary_io_u])
"""

binarización con > 128 (umbral=128)
30 epochs
N (total samples) = 4800
train_samples = 3600 + 1222
batch size = 2
test_size: 500 samples
dropout 0.4

IOU total: 382.55
IOU avg: 0.7651
training time = 5.51h

Nota:	
	Data augmentation
	Se emoieza a notar claramente el efecto perjudicial del dropout
	Resultados poco consistentes
	  
//-----------------------------------------------------------------------------------------//
###  - 30 -
10-06-22
model0V2_aug_BCE_04_2

"""
optimizer = keras.optimizers.Adam()
				copiled_loss = weighted_pixelwise_crossentropy_tf
                loss = 'binary_crossentropy',
                metrics = [binary_io_u])
"""

binarización con > 128 (umbral=128)
30 epochs
N (total samples) = 4800
train_samples = 3600 + 1222
batch size = 2
test_size: 500 samples
dropout 0.4

IOU total: 398.873 (*)
IOU avg: 0.79774
training time = 5.51h

Nota:	
	Data augmentation
	Se empieza a notar claramente el efecto perjudicial del dropout
	Resultados poco consistentes
	(*) Estos resultados son más representativos porque el entrenamiento no ha tendio nada raro.
	Aun asi los rsultados están mas degradados que con menos drpout
	  
//-----------------------------------------------------------------------------------------//
###  - 30 -
10-06-22
model0V2_aug_BCE_00_3

"""
optimizer = keras.optimizers.Adam()
				copiled_loss = weighted_pixelwise_crossentropy_tf
                loss = 'binary_crossentropy',
                metrics = [binary_io_u])
"""

binarización con > 128 (umbral=128)
30 epochs
N (total samples) = 4800
train_samples = 3600 + 1222
batch size = 2
test_size: 500 samples
dropout 0.0

IOU total: 400.57 (*)
IOU avg: 0.8011
training time = 5.51h

Nota:	
	Data augmentation
	  
//-----------------------------------------------------------------------------------------//
###  - 30 -
10-06-22
model0V2_aug_BCE_01_3

"""
optimizer = keras.optimizers.Adam()
				copiled_loss = weighted_pixelwise_crossentropy_tf
                loss = 'binary_crossentropy',
                metrics = [binary_io_u])
"""

binarización con > 128 (umbral=128)
30 epochs
N (total samples) = 4800
train_samples = 3600 + 1222
batch size = 2
test_size: 500 samples
dropout 0.1

IOU total: 396.36 (*)
IOU avg: 0.79273
training time = 5.51h

Nota:	
	Data augmentation
	  
//-----------------------------------------------------------------------------------------//
###  - 30 -
10-06-22
model0V2_aug_BCE_02_3

"""
optimizer = keras.optimizers.Adam()
				copiled_loss = weighted_pixelwise_crossentropy_tf
                loss = 'binary_crossentropy',
                metrics = [binary_io_u])
"""

binarización con > 128 (umbral=128)
30 epochs
N (total samples) = 4800
train_samples = 3600 + 1222
batch size = 2
test_size: 500 samples
dropout 0.2

IOU total: 398.25 (*)
IOU avg: 0.79650
training time = 5.51h

Nota:	
	Data augmentation
	  
//-----------------------------------------------------------------------------------------//
###  - 31 -
14-06-22
model0V2_aug_BCE_03_3

"""
optimizer = keras.optimizers.Adam()
				copiled_loss = weighted_pixelwise_crossentropy_tf
                loss = 'binary_crossentropy',
                metrics = [binary_io_u])
"""

binarización con > 128 (umbral=128)
30 epochs
N (total samples) = 4800
train_samples = 3600 + 1222
batch size = 2
test_size: 500 samples
dropout 0.3

IOU total: 397.61 (*)
IOU avg: 0.79521
training time = 5.51h

Nota:	
	Data augmentation
	  
//-----------------------------------------------------------------------------------------//
###  - 32 -
14-06-22
model0V2_aug_BCE_04_3

"""
optimizer = keras.optimizers.Adam()
				copiled_loss = weighted_pixelwise_crossentropy_tf
                loss = 'binary_crossentropy',
                metrics = [binary_io_u])
"""

binarización con > 128 (umbral=128)
30 epochs
N (total samples) = 4800
train_samples = 3600 + 1222
batch size = 2
test_size: 500 samples
dropout 0.4

IOU total: 398.48 (*)
IOU avg: 0.7969
training time = 5.51h

Nota:	
	Data augmentation
	  
//-----------------------------------------------------------------------------------------//
###  - 33 -
14-06-22
model0_PReLU_aug_BCE_03_1

"""
optimizer = keras.optimizers.Adam()
				copiled_loss = weighted_pixelwise_crossentropy_tf
                loss = 'binary_crossentropy',
                metrics = [binary_io_u])
"""

binarización con > 128 (umbral=128)
30 epochs
N (total samples) = 4800
train_samples = 3600 + 1222
batch size = 2
test_size: 500 samples
dropout 0.3

IOU total: 406.08 (*)
IOU avg: 0.81215
training time = 6.634h

Nota:	
	Data augmentation
	  
//-----------------------------------------------------------------------------------------//
###  - 33 -
14-06-22
model0_PReLU_aug_BCE_03_2

"""
optimizer = keras.optimizers.Adam()
				copiled_loss = weighted_pixelwise_crossentropy_tf
                loss = 'binary_crossentropy',
                metrics = [binary_io_u])
"""

binarización con > 128 (umbral=128)
30 epochs
N (total samples) = 4800
train_samples = 3600 + 1222
batch size = 2
test_size: 500 samples
dropout 0.3

IOU total: 405.84 (*)
IOU avg: 0.81169
training time = 6.634h

Nota:	
	Data augmentation
	  
//-----------------------------------------------------------------------------------------//
###  - 34 -
14-06-22
model0_PReLU_aug_BCE_03_3

"""
optimizer = keras.optimizers.Adam()
				copiled_loss = weighted_pixelwise_crossentropy_tf
                loss = 'binary_crossentropy',
                metrics = [binary_io_u])
"""

binarización con > 128 (umbral=128)
30 epochs
N (total samples) = 4800
train_samples = 3600 + 1222
batch size = 2
test_size: 500 samples
dropout 0.3

IOU total: 402.86 (*)
IOU avg: 0.8057
training time = 6.634h

Nota:	
	Data augmentation
	  
//-----------------------------------------------------------------------------------------//
###  - 35 -
17-06-22
model0_PReLU_aug_DICE_03_1

"""
optimizer = keras.optimizers.Adam()
				copiled_loss = weighted_pixelwise_crossentropy_tf
                loss = 'binary_crossentropy',
                metrics = [binary_io_u])
"""

binarización con > 128 (umbral=128)
30 epochs
N (total samples) = 4800
train_samples = 3600 + 1222
batch size = 2
test_size: 500 samples
dropout 0.3

IOU total: 382.07
IOU avg: 0.76415
training time = 6.65h

Nota:	
	Data augmentation
	  
//-----------------------------------------------------------------------------------------//
###  - 36 -
18-06-22
model0V2_aug_FOCAL_o92_1o5_04_1

"""
optimizer = keras.optimizers.Adam()
				copiled_loss = weighted_pixelwise_crossentropy_tf
                loss = 'binary_crossentropy',
                metrics = [binary_io_u])
"""

binarización con > 128 (umbral=128)
30 epochs
N (total samples) = 4800
train_samples = 3600 + 1222
batch size = 2
test_size: 500 samples
dropout 0.4

IOU total: 368.38
IOU avg: 0.7368
training time = 5.81h

Nota:	
	Data augmentation
	Demasiados flasos positivos, pruebo a bajar gamma a ~0.5~ / 1.0 por problemas de 
		convergencia, supongo que el gradiente es extraño para gammas bajas. 
//-----------------------------------------------------------------------------------------//
###  - 36 -
18-06-22
model0V2_aug_FOCAL_o92_1o_04_1

"""
optimizer = keras.optimizers.Adam()
				copiled_loss = weighted_pixelwise_crossentropy_tf
                loss = 'binary_crossentropy',
                metrics = [binary_io_u])
"""

binarización con > 128 (umbral=128)
30 epochs
N (total samples) = 4800
train_samples = 3600 + 1222
batch size = 2
test_size: 500 samples
dropout 0.4

IOU total: 373.74
IOU avg: 0.74749
training time = 5.81h

Nota:	
	Data augmentation
	Demasiados flasos positivos, pruebo a bajar alpha a 0.5 
//-----------------------------------------------------------------------------------------//
###  - 37 -
18-06-22
modelV2_aug_FOCAL_o5_1o_04_1

"""
optimizer = keras.optimizers.Adam()
				copiled_loss = weighted_pixelwise_crossentropy_tf
                loss = 'binary_crossentropy',
                metrics = [binary_io_u])
"""

binarización con > 128 (umbral=128)
30 epochs
N (total samples) = 4800
train_samples = 3600 + 1222
batch size = 2
test_size: 500 samples
dropout 0.4

IOU total: 402.69684
IOU avg: 0.8054
training time = 5.81h

Nota:	
	Data augmentation
	Mucha diferencia entre el recall yla precisión. Pruebo a bajar ligeramente gamma.
	Muy buenos resultados
//-----------------------------------------------------------------------------------------//
###  - 37 -
18-06-22
model0_PReLU_aug_FOCAL_o5_1o_03_1

"""
optimizer = keras.optimizers.Adam()
				copiled_loss = weighted_pixelwise_crossentropy_tf
                loss = 'binary_crossentropy',
                metrics = [binary_io_u])
"""

binarización con > 128 (umbral=128)
30 epochs
N (total samples) = 4800
train_samples = 3600 + 1222
batch size = 2
test_size: 500 samples
dropout 0.4

IOU total: 399.1
IOU avg: 0.79821
training time = 5.81h

Nota:	
	Data augmentation
	Repetir este, iba bien y se fue a la mierda un poco
//-----------------------------------------------------------------------------------------//
###  - 38 -
18-06-22
model0_PReLU_aug_FOCAL_o5_1o_04_2

"""
optimizer = keras.optimizers.Adam()
				copiled_loss = weighted_pixelwise_crossentropy_tf
                loss = 'binary_crossentropy',
                metrics = [binary_io_u])
"""

binarización con > 128 (umbral=128)
30 epochs
N (total samples) = 4800
train_samples = 3600 + 1222
batch size = 2
test_size: 500 samples
dropout 0.4

IOU total: 
IOU avg: 
training time = 5.81h

Nota:	
	Data augmentation
	Malos resultados otra vez, vuelvo a probar con ReLUs y Focal loss 0.5_1.0
//-----------------------------------------------------------------------------------------//
###  - 39 -
18-06-22
modelV2_PReLU_aug_FOCAL_o6_1o_03_1

"""
optimizer = keras.optimizers.Adam()
				copiled_loss = weighted_pixelwise_crossentropy_tf
                loss = 'binary_crossentropy',
                metrics = [binary_io_u])
"""

binarización con > 128 (umbral=128)
30 epochs
N (total samples) = 4800
train_samples = 3600 + 1222
batch size = 2
test_size: 500 samples
dropout 0.4

IOU total: 
IOU avg: 0.81
training time = 5.81h

Nota:	
	
//-----------------------------------------------------------------------------------------//
###  - 40 -
18-06-22
modelV2_PReLU_aug_FOCAL_o6_1o_03_1

"""
optimizer = keras.optimizers.Adam()
				copiled_loss = weighted_pixelwise_crossentropy_tf
                loss = 'binary_crossentropy',
                metrics = [binary_io_u])
"""

binarización con > 128 (umbral=128)
30 epochs
N (total samples) = 4800
train_samples = 3600 + 1222
batch size = 2
test_size: 500 samples
dropout 0.3

IOU total: 
IOU avg: 0.81
training time = 5.81h

Nota:	
	
//-----------------------------------------------------------------------------------------//
###  - 40 -
18-06-22
modelV2_PReLU_aug_FOCAL_o5_1o1_03_1

"""
optimizer = keras.optimizers.Adam()
				copiled_loss = weighted_pixelwise_crossentropy_tf
                loss = 'binary_crossentropy',
                metrics = [binary_io_u])
"""

binarización con > 128 (umbral=128)
30 epochs
N (total samples) = 4800
train_samples = 3600 + 1222
batch size = 2
test_size: 500 samples
dropout 0.3

IOU total: 
IOU avg: 0.8083
training time = 5.81h

Nota:	
	
//-----------------------------------------------------------------------------------------//







